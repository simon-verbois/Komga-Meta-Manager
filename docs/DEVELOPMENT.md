# Development Guide - Komga Meta Manager

## ğŸ—ï¸ Architecture

The project follows a modular architecture with clear separation of responsibilities:

```
src/manga_manager/
â”œâ”€â”€ constants.py          # Centralized constants
â”œâ”€â”€ config.py             # Configuration and validation
â”œâ”€â”€ main.py               # Main entry point
â”œâ”€â”€ metrics.py            # Metrics collection
â”œâ”€â”€ models.py             # Pydantic data models
â”œâ”€â”€ komga_client.py       # Client for Komga API
â”œâ”€â”€ processor.py          # Main processing logic
â”œâ”€â”€ providers/            # Metadata providers
â”‚   â”œâ”€â”€ base.py          # Common interface
â”‚   â””â”€â”€ anilist_provider.py  # AniList implementation
â””â”€â”€ translators/          # Translators
    â”œâ”€â”€ base.py          # Common interface
    â”œâ”€â”€ google_translator.py  # Google Translate implementation
    â””â”€â”€ deepl_translator.py   # DeepL implementation
```

## ğŸ§ª Tests

### Test Structure

Tests are organized in the `tests/` folder:

```
tests/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ conftest.py           # Fixtures and configuration
â”œâ”€â”€ pytest.ini           # Pytest configuration
â”œâ”€â”€ test_processor.py    # Processing logic tests
â”œâ”€â”€ test_metrics.py      # Metrics tests
â””â”€â”€ test_config.py       # Configuration validation tests (TODO)
```

### Running Tests

```bash
# All tests
pytest

# Specific tests
pytest tests/test_processor.py

# With coverage
pytest --cov=manga_manager --cov-report=html

# Slow tests only
pytest -m slow

# Unit tests only (default)
pytest -m unit
```

### Writing Tests

#### Principles
- Use `pytest` as testing framework
- Organize tests in classes with descriptive names
- Use fixtures for common configuration
- Mock external calls (APIs, filesystem)
- Test error cases as well as happy paths

#### Test Example

```python
import pytest
from unittest.mock import Mock

def test_example():
    # Arrange
    component = Component()
    config = Mock()

    # Act
    result = component.process(config)

    # Assert
    assert result.success is True
    assert result.data is not None
```

## ğŸ”§ Development

### Environment

1. Create a virtual environment:
```bash
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
# or
.venv\Scripts\activate     # Windows
```

2. Install dependencies:
```bash
pip install -r requirements.txt
pip install -r requirements-dev.txt
```

3. Install in development mode:
```bash
pip install -e .
```

### Pre-commit hooks

To maintain code quality:

```bash
pip install pre-commit
pre-commit install
```

Hooks verify:
- Code formatting (black)
- Import sorting (isort)
- Linting (flake8)
- Automated tests

### Debugging

#### Debug mode

Enable debug mode in configuration:

```yaml
system:
  debug: true
```

#### Detailed logs

Add temporary logs:

```python
import logging
logger = logging.getLogger(__name__)
logger.debug(f"Processing: {variable}")
```

#### Breakpoints

Use `pdb` for interactive debugging:

```python
import pdb; pdb.set_trace()
```

## ğŸš€ Deployment

### Docker

The project uses Docker for production:

```bash
# Build
docker build -t komga-meta-manager .

# Run
docker run --rm komga-meta-manager
```

### Local Development

To test locally:

```bash
# Dry-run mode
python -m manga_manager.main

# With custom configuration
python -m manga_manager.main --config /path/to/config.yml
```

## ğŸ“‹ Best Practices

### Code

- **Type hints**: Always use type annotations
- **Docstrings**: Document all public functions
- **PEP 8**: Respect Python style conventions
- **Fail fast**: Check preconditions at function start

### Error Handling

- **Specific exceptions**: Create custom exceptions when needed
- **Clear messages**: Provide informative error messages
- **Logging**: Log errors with context
- **Recovery**: Implement recovery strategies when possible

### Performance

- **Cache**: Use cache to avoid repeated calls
- **Metrics**: Profile bottlenecks with metrics
- **Async**: Consider asyncio for massive I/O operations (future)

## ğŸ¤ Contribution

### Workflow

1. Create a feature branch: `git checkout -b feature/feature-name`
2. Write tests for the new functionality
3. Implement the functionality
4. Verify all tests pass
5. Submit PR with detailed description

### Commit Conventions

- `feat`: New feature
- `fix`: Bug fix
- `docs`: Documentation
- `test`: Tests
- `refactor`: Refactoring without functional changes
- `perf`: Performance improvement
- `chore`: Maintenance

### Reviews

- All PRs must be reviewed
- Tests must pass on CI
- Coverage must be maintained > 80%

## ğŸ“Š Monitoring

### Key Metrics

- Average processing time per series
- API call success rate
- Cache hit rates
- Error count by type

### Alerts

- API failure rate > 10%
- Processing time > 2x average
- Cache hit rate < 50%

See `metrics.py` for complete implementation.
